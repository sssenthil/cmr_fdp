# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VQMEPst81GyLFgfgbCAz7P_r5524V6XK
"""

pip install groq

pip install llama-index-llms-groq

pip install llama-index

from llama_index.llms.groq import Groq

GROQ_API_KEY="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"

llm=(Groq(model="llama3-70b-8192", api_key="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")



llm=Groq(model="llama3-70b-8192", api_key="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")

response=llm.complete("Explain the importance of low latency LLMS")

print(response)

from llama_index.core.llms import ChatMessage
message=[
    ChatMessage(role="system", content="Your a private with a colorful personality"),
    ChatMessage(role="user", content="What is your name")
]
resp=llm.chat(message)



print(resp)

from llama_index.core.llms import ChatMessage
message=[
    ChatMessage(role="system", content="Which domain will be a trends in future"),
    ChatMessage(role="user", content="What is Domain name")
]
resp=llm.chat(message)

print(resp)

response=llm.stream_complete("Explain the importance of low latency LLMs)
for r in response:
  print(r.delta,end="")

response = llm.stream_complete("Explain the importance of low latency LLMs")
for r in response:
  print(r.delta,end="")

pip install https://colab.research.google.com/drive/1s6mvjR1aPB3h2MQOaIdnWttVnc8f-2ey?usp=sharing

pip install llama-index llama-index-embeddings-huggingface llama-index-llms-groq

import logging
import sys
from google.colab import userdata
import os
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
# from llama_index.llms.openai import OpenAI
from llama_index.llms.groq import Groq

logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)
logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))

!mkdir data
!wget -O data/essay.txt https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt

documents = SimpleDirectoryReader("data").load_data()

embed_model = HuggingFaceEmbedding(model_name="BAAI/bge-small-en-v1.5")

GROQ_API_KEY = "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"

# os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')

# llm = OpenAI(
#     model="gpt-3.5-turbo",
#     api_key=userdata.get('sk-o60aCUo7BIVicqzB9moWRGmmqi1cipD_DxNfLgYvdRT3BlbkFJBEdMcAOAsfbQoVnuEbEMDBjEm8WMrz16hteNIyjqUA')
# )

llm = Groq(
   model="llama3-70b-8192",
   api_key="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"
)

Settings.embed_model = embed_model
Settings.llm = llm

index = VectorStoreIndex.from_documents(documents)

query_engine = index.as_query_engine()

response = query_engine.query("What did the author do growing up?")
print(response)

response = query_engine.query("What is the customer Id")
print(response)